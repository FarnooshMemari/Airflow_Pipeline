### ğŸ“Œ Customersâ€“Orders Data Pipeline (Airflow + Postgres + PySpark)

This project implements an end-to-end **Airflow pipeline** that ingests, processes, merges, and analyzes two related datasets using workflow orchestration and containerized deployment.

---

### âœ… What the Pipeline Does

- **Scheduled DAG (@daily)** orchestrated using Airflow.
- **Ingests two datasets**: `customers.csv` and `orders.csv`.
- **Parallel processing** using `TaskGroup` for ingestion and transformation.
- **Data transformations**:
  - Cleaned customer names and formatted dates.
  - Calculated `order_amount = quantity * unit_price`.
- **Merged datasets** on `customer_id`.
- **Loaded final data** into PostgreSQL table `public.customer_orders`.
- **Analysis step**:
  - Used PySpark to compute top customers by spending.
  - Saved visualization to `reports/top_customers.png`.
- **Cleanup task** removes intermediate files after execution.
- **PostgreSQL connection is auto-created via code** (no manual UI setup needed).

---

### ğŸ›  Technologies Used

| Tool        | Purpose                              |
|-------------|----------------------------------------|
| Airflow     | DAG scheduling and orchestration       |
| Pandas      | Data ingestion & transformation        |
| TaskGroups  | Parallel execution                     |
| PostgreSQL  | Data warehouse                         |
| PySpark     | Analysis & visualization (bonus)       |
| Docker      | Containerized deployment               |

---

## ğŸš€ Pipeline Overview

```
start
 â”œâ”€ ensure_dirs
 â”œâ”€ ingest/
 â”‚    â”œâ”€ ingest_customers
 â”‚    â””â”€ ingest_orders
 â”œâ”€ transform/
 â”‚    â”œâ”€ transform_customers
 â”‚    â””â”€ transform_orders
 â”œâ”€ merge_and_load
 â”œâ”€ spark_analysis  (bonus â€“ PySpark + visualization)
 â”œâ”€ cleanup
 â””â”€ end
```

---

## ğŸ“ Repository Structure

```
airflow-pipeline/
â”œâ”€â”€ docker-compose.yml         # Airflow + Postgres services
â”œâ”€â”€ Dockerfile                 # Custom Airflow image
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ .env                       # Environment variables (ignored in GitHub)
â”œâ”€â”€ README.md
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ pipeline.py            # Airflow DAG definition
â”œâ”€â”€ include/
â”‚   â””â”€â”€ data/
â”‚       â”œâ”€â”€ raw/               # Input CSV files (customers.csv, orders.csv)
â”‚       â””â”€â”€ stage/             # Temp files generated per run
â”œâ”€â”€ jars/
â”‚   â””â”€â”€ postgresql-42.7.4.jar  # JDBC driver for Spark (added manually)
â”œâ”€â”€ reports/
â”‚   â””â”€â”€ top_customers.png      # Visualization generated by Spark
â””â”€â”€ sql/
    â””â”€â”€ create_target_tables.sql (optional)
```

---

## ğŸ¯ Features Covered (as required):

âœ… Two related datasets  
âœ… Parallel ingestion & transformation  
âœ… DAG scheduling + orchestration  
âœ… Loading to PostgreSQL  
âœ… Cleanup step  
âœ… Documentation + screenshots  
âœ… Bonus: PySpark analysis + visualization
